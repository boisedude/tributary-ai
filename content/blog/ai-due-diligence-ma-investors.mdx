---
title: "AI Due Diligence: What M&A Teams and Investors Need to Assess"
date: "2025-07-16"
excerpt: "AI due diligence checklist for M&A: 15 questions to separate genuine AI capabilities from marketing hype. Don't overpay for vaporware."
author: "Tributary"
tags: ["Due Diligence", "M&A", "Investment", "AI Assessment"]
image: "/blog/ai-due-diligence-ma.webp"
---

Every company you evaluate now claims to be "AI-powered" or "leveraging machine learning." Most are lying, some are delusional, and a few actually have valuable AI capabilities.

The problem for M&A teams and investors: standard due diligence processes weren't built to distinguish between these categories. Technical due diligence checks for working software. Financial due diligence validates revenue. Neither tells you if the "AI" is real, sustainable, or valuable.

Companies have learned that "AI" commands premium valuations. They've also learned that most acquirers and investors can't tell the difference between sophisticated AI systems and basic automation rebranded as machine learning.

Here's how to assess AI capabilities accurately during due diligence, separate real value from marketing, and avoid overpaying for glorified spreadsheets.

## Red Flags: What Fake AI Looks Like

Start with red flags. These don't definitively prove AI claims are false, but they indicate high risk of exaggeration or misunderstanding.

**Marketing Red Flags**:

**Vague AI Claims**: "We use AI to enhance customer experience" without specific use cases, capabilities, or measurable outcomes. Real AI implementations are concrete and measurable.

**AI Everywhere**: Claims that every product feature and business process uses AI. This signals the company doesn't understand what AI actually is. Real AI deployments are targeted at specific problems where AI provides advantage.

**Buzzword Density**: Press releases and pitch decks heavy on "machine learning," "neural networks," "deep learning," and "artificial intelligence" but light on actual capabilities and results. This is often compensation for lack of substance.

**No AI Team**: The company claims significant AI capabilities but has no data scientists, ML engineers, or AI-focused developers on staff. Someone has to build and maintain AI systems.

**Technical Red Flags**:

**Rules-Based Systems Called AI**: If-then logic, decision trees, and expert systems aren't AI. If you dig into the "AI" and find it's just automated business rules, it's not what they're claiming.

**Purchased AI Without Customization**: The company licensed a vendor's AI tool but hasn't trained models on their data, customized for their use case, or built proprietary capabilities. They're a customer, not an AI company.

**No Data Infrastructure**: Real AI requires data pipelines, storage, processing capabilities, and quality controls. If the company doesn't have data infrastructure, they don't have AI.

**Static Models Never Updated**: AI models degrade over time as patterns change. If models were trained once and never retrained, they're not production AI systems.

**No Model Performance Tracking**: Real AI deployments monitor model accuracy, drift, and business impact continuously. If they can't show you performance metrics, they're not seriously operating AI in production.

**Organizational Red Flags**:

**AI as Marketing, Not Operations**: The AI is mentioned in marketing materials but not discussed by operations teams. This means it's a sales tool, not an operational capability. This often indicates [pilot scaling challenges](/blog/why-ai-pilots-fail-to-scale) that prevent AI from becoming operationally meaningful.

**No Ownership or Accountability**: Nobody can clearly explain who owns AI systems, who maintains them, or how they impact business metrics. This signals AI isn't actually integrated into operations.

**Separate from Core Product**: The AI is a side feature or bolt-on, not integrated into core product functionality or business processes. This suggests it's peripheral, not fundamental.

---

**Ready to assess your organization's AI readiness?** Tributary's Strategic Assessment evaluates your technology, data, people, and processes to identify what's blocking your AI success. [Schedule your assessment →](/assessment)

---

## Green Flags: What Real AI Looks Like

**Technical Green Flags**:

**Specific, Measurable Capabilities**: The company can articulate exactly what AI does, how it works, and what business outcomes it drives. "We use ML to predict customer churn 60 days in advance with 78% accuracy, enabling proactive retention campaigns that reduce churn 23%" is a green flag.

**Proprietary Data Assets**: Real AI advantage comes from unique data. If the company has accumulated valuable proprietary datasets and uses them to train models competitors can't replicate, that's genuine capability.

**Custom Model Development**: They're training or fine-tuning models on their data for their specific use cases, not just using off-the-shelf tools. This indicates real AI expertise and differentiation.

**Production AI Infrastructure**: They have MLOps pipelines, model monitoring, A/B testing frameworks, and automated retraining. This infrastructure only exists when AI is core to operations.

**Measurable AI Impact**: They can show clear before/after metrics demonstrating AI impact on revenue, costs, or key performance indicators. Real AI delivers measurable results.

**Organizational Green Flags**:

**AI Talent Depth**: Multiple data scientists, ML engineers, or AI specialists who've been with the company 12+ months. Retention indicates real AI work, not just hiring for optics.

**Executive AI Literacy**: Leadership can discuss AI capabilities intelligently without buzzwords. They understand what their AI does, how it creates value, and what its limitations are.

**AI in Core Metrics**: AI performance shows up in board decks, investor updates, and operational dashboards. If it's important, it gets measured and reported.

**Cross-Functional Integration**: AI isn't isolated in a data science team. It's integrated into product, operations, or sales processes with clear ownership and accountability.

**Business Model Green Flags**:

**AI Enables the Business Model**: The business wouldn't work without AI, or would work far worse. The AI isn't a feature—it's fundamental to competitive advantage.

**Network Effects from Data**: The more customers they serve, the better their AI gets (more data for training). This creates defensibility.

**Measurable ROI**: They can articulate AI's return on investment. How much does AI capability contribute to revenue or reduce costs? Real AI users know this number.

## Technical Assessment: What to Actually Verify

Don't rely on executive presentations. Get technical access and verify claims.

**Data Assessment**:

**Data Quality and Volume**: Examine the actual data used for AI. Is it clean, labeled, representative? Does volume support the claimed capabilities? Poor data quality undermines all AI claims.

**Data Lineage and Governance**: Can they trace where data comes from, how it's processed, and who has access? Good data governance indicates mature AI operations.

**Data Uniqueness**: Is their data genuinely proprietary and valuable, or is it commodity data anyone could acquire? Unique data creates competitive moats; commodity data doesn't.

**Labeling and Ground Truth**: For supervised learning, how are labels generated? How is accuracy validated? If they can't explain this, their AI likely doesn't work as claimed.

**Model Assessment**:

**Model Architecture and Approach**: What types of models are they using? Are they appropriate for the problem? Over-engineering (using deep learning for simple problems) or under-engineering (using linear regression for complex ones) both signal issues.

**Training and Validation Methodology**: How do they split training/validation/test data? How do they prevent overfitting? What metrics do they optimize for? Proper methodology is essential for reliable AI.

**Production Performance vs. Lab Performance**: Lab accuracy is often 10-20 points higher than production. Get production metrics, not development metrics.

**Model Versioning and Experimentation**: Can they show you model version history, experiments run, and results? Active experimentation indicates ongoing improvement.

**Deployment Assessment**:

**Inference Infrastructure**: How are models deployed? What's the latency, throughput, reliability? Production AI needs production infrastructure.

**Monitoring and Alerting**: What metrics are monitored? How do they detect model degradation or failure? If they're not monitoring, it's not production.

**Rollback and Fail-Safe Mechanisms**: What happens when models fail? Can they quickly revert to previous versions or fall back to non-AI processes? Production systems need safety mechanisms.

**Integration Points**: How does AI integrate with existing systems and workflows? Complex, fragile integrations create risk.

## Business Impact Assessment

Technical capability means nothing if it doesn't create business value.

**Revenue Impact**:

**Direct Revenue Attribution**: For AI that drives revenue (recommendations, pricing, lead scoring), can they demonstrate clear lift? Run A/B tests? Show cohort analysis?

**Customer Retention**: Does AI measurably improve retention, lifetime value, or expansion revenue? Get actual numbers, not anecdotes.

**Conversion Improvements**: If AI improves conversion (marketing, sales, product), quantify the impact with proper controls.

**Cost Impact**:

**Automation Savings**: If AI automates work, calculate actual labor savings net of AI development and operation costs. Many "cost saving" AI projects cost more than they save.

**Efficiency Gains**: Measure productivity improvements in processes AI augments. Are people handling more volume, higher complexity, or both?

**Infrastructure Costs**: What does it cost to run AI systems? Include compute, storage, API costs, and personnel. High costs can eliminate margins.

**Competitive Advantage**:

**Differentiation**: Does AI enable capabilities competitors can't match? If competitors could easily replicate the AI, it's not a defensible advantage.

**Time to Replicate**: How long would it take a competitor to build equivalent AI capabilities? Longer timelines indicate stronger moats.

**Data Moats**: Are there accumulating data advantages that strengthen over time? This is the most defensible AI advantage.

**Strategic Risks**:

**Vendor Dependency**: Are critical AI capabilities dependent on third-party vendors or APIs (OpenAI, Google, etc.)? This creates risk if terms change or access is restricted.

**Key Person Risk**: Is AI capability concentrated in one or two individuals? High turnover risk in AI talent makes this critical.

**Regulatory Risk**: Are AI applications subject to emerging regulation (hiring, credit, healthcare, financial services)? Factor in compliance costs and restrictions.

## Team Capability Assessment

AI systems require ongoing development and maintenance. Assess whether the team can sustain and advance capabilities.

**Technical Team Evaluation**:

**Depth and Experience**: Don't just count data scientists. Evaluate their backgrounds, publication records, previous companies, and tenure. Strong teams have mix of PhDs and practitioners.

**Full Stack Coverage**: Real AI teams need more than data scientists. Look for ML engineers (deploy models), data engineers (build pipelines), and domain experts (understand problems).

**Retention and Culture**: High turnover in AI teams indicates problems. Talk to team members about culture, resources, and leadership support.

**Leadership Understanding**: Do engineering and product leaders understand AI deeply enough to make good decisions? Weak leadership undermines strong teams.

**Process Maturity**:

**Development Processes**: Do they have established processes for experimentation, validation, and deployment? Or is it ad hoc?

**Documentation**: Are models, experiments, and decisions documented? Can the team explain past decisions and results?

**Knowledge Sharing**: How is AI knowledge distributed across the team? Single points of failure create risk.

**Continuous Learning**: Is the team staying current with AI advances or working with outdated approaches?

## Integration and Scalability Assessment

Evaluate whether AI capabilities can scale and integrate into acquirer's operations.

**Technical Integration**:

**Architecture Compatibility**: Will AI systems work with acquirer's infrastructure or require complete rebuilds?

**Data Integration**: Can you combine datasets to improve models, or are there technical or legal barriers?

**Technology Stack**: Are they using standard, supportable technology or exotic tools that create maintenance burden?

**Operational Integration**:

**Process Compatibility**: Do AI-enabled processes align with acquirer's operating model or conflict?

**Organizational Fit**: Can AI team integrate into larger org or does culture clash create risk?

**Scalability**: Can AI systems handle significantly larger volume or are they at technical limits?

## Valuation Implications

Real AI capabilities deserve premium valuations. Fake AI deserves discounts for misrepresentation.

**Premium Justified If**:
- AI creates defensible competitive advantages (data moats, unique capabilities)
- AI demonstrably drives revenue or reduces costs with clear ROI
- AI capabilities would take competitors 18+ months to replicate
- Strong team with deep expertise and low turnover risk
- Robust infrastructure supporting continued innovation

**Discount Required If**:
- AI claims are exaggerated or misleading
- AI is peripheral to business value, not core
- High dependency on vendors or key individuals
- No clear path to AI ROI or competitive advantage
- Technical debt or infrastructure gaps require significant investment

**Walk Away If**:
- AI claims are fundamentally false (rules-based systems called AI)
- No real AI team or capability
- Can't demonstrate any business impact from AI
- Regulatory risks outweigh potential benefits

## Due Diligence Checklist

**Request These Materials**:
- Model architecture documentation and performance metrics
- Data pipeline and governance documentation
- AI team org chart with backgrounds and tenure
- Infrastructure costs (compute, APIs, storage, personnel)
- A/B test results and business impact analysis
- Model monitoring dashboards and alerting systems
- Code repository access for technical review
- Customer case studies demonstrating AI value

**Conduct These Interviews**:
- Data science team (technical depth, capability assessment)
- Engineering leadership (integration, scalability, technical debt)
- Product leadership (AI impact on roadmap and strategy)
- Operations teams (AI impact on processes and metrics)
- Customers (actual value delivered by AI features)

**Perform These Tests**:
- Technical code review by AI experts
- Data quality assessment
- Model performance validation with fresh data
- Infrastructure load testing
- Competitive AI capability benchmarking

## The Path Forward

AI is becoming a standard component of business value, but most companies claiming AI capabilities are overselling, misunderstanding, or outright fabricating them.

Standard due diligence misses this because it wasn't designed to assess AI. Technical reviews check if code works. Financial reviews validate revenue. Neither determines if "AI" claims are legitimate or valuable.

The acquirers and investors who win in this environment are those who build AI assessment capabilities—either through specialized team members or trusted advisors who can distinguish real from fake.

The companies that deserve AI-driven valuations are easy to identify once you know what to look for: they have real AI capabilities, delivering measurable business impact, with defensible advantages and teams capable of sustaining innovation.

Everything else is marketing.

---

## Take the Next Step

Standard due diligence was not built to assess AI claims—you need specialized expertise to separate genuine value from marketing hype. Tributary helps mid-market companies navigate AI implementation with clarity and confidence.

**[Take our free AI Readiness Quiz →](/quiz)** to assess your own organization's AI capabilities, or **[schedule a consultation](/contact)** to get independent AI due diligence expertise for your M&A or investment evaluation.
